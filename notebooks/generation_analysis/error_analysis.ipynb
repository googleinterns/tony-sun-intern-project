{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import spacy\n",
    "from emoji import UNICODE_EMOJI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 16.7 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sys\n",
    "sys.path.append('../../neutral_generation/')\n",
    "from is_gendered import is_gendered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('male', 'female')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_gendered('he'), is_gendered('she')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_target(eval_set):\n",
    "    with open(f'../../evaluation/{eval_set}/source.txt', 'r') as f:\n",
    "        source = f.readlines()\n",
    "    \n",
    "    with open(f'../../evaluation/{eval_set}/target.txt', 'r') as f:\n",
    "        target = f.readlines()\n",
    "        \n",
    "    return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "source, target = get_source_target('gendered_test_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 500)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(source), len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter = Counter()\n",
    "\n",
    "for sent in source:\n",
    "    tokens = sent.lower().split(' ')\n",
    "    for token in tokens:\n",
    "        token_counter[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2522"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num of unique tokens\n",
    "len(token_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6427"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens = 0\n",
    "for token in token_counter:\n",
    "    total_tokens += token_counter[token]\n",
    "total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 274),\n",
       " ('a', 180),\n",
       " ('to', 158),\n",
       " ('he', 146),\n",
       " ('she', 136),\n",
       " ('and', 131),\n",
       " ('her', 122),\n",
       " ('in', 97),\n",
       " ('his', 95),\n",
       " ('was', 93),\n",
       " ('i', 88),\n",
       " ('of', 82),\n",
       " ('for', 63),\n",
       " ('is', 57),\n",
       " ('on', 50),\n",
       " ('that', 47),\n",
       " ('my', 46),\n",
       " ('with', 46),\n",
       " ('has', 38),\n",
       " ('you', 38)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def get_difference(old_sent: str, new_sent: str):\n",
    "    old_words = old_sent.strip().split(' ')\n",
    "    new_words = new_sent.strip().split(' ')\n",
    "    \n",
    "    removed_words = list(set(old_words) - set(new_words))\n",
    "    added_words = list(set(new_words) - set(old_words))\n",
    "#     removed_words = list(word for i, word in enumerate(old_words) if word != new_words[i])\n",
    "#     added_words = list(word for i, word in enumerate(new_words) if word != old_words[i])\n",
    "    \n",
    "    return removed_words, added_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_difference_sent_list(generation, target):\n",
    "    changes = list()\n",
    "    num_changes = 0\n",
    "\n",
    "    for i in range(500):\n",
    "        diffs = get_difference(generation[i], target[i])\n",
    "    #     if len(diffs[0]) != len(diffs[1]):\n",
    "    #         print(diffs)\n",
    "    #         print(generation[i], target[i])\n",
    "\n",
    "        changes.append(diffs)\n",
    "        num_changes = num_changes + len(diffs[1])\n",
    "    \n",
    "    return changes, num_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "743"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1590 / 795 / 795 for perfect count\n",
    "changes, num_changes = get_difference_sent_list(generation=source, target=target)\n",
    "num_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximation, isn't perfect\n",
    "# e.g. set subtraction for word diff, \n",
    "# e.g. calculation of percent of \"correct changes\" bc target changes aren't necessarily in source changes\n",
    "\n",
    "def get_changes(eval_set, generation):\n",
    "    with open(f'../../evaluation/{eval_set}/generations/{generation}/generation.txt', 'r') as f:\n",
    "        generation = f.readlines()\n",
    "    \n",
    "    with open(f'../../evaluation/{eval_set}/source.txt', 'r') as f:\n",
    "        source = f.readlines()\n",
    "    \n",
    "    with open(f'../../evaluation/{eval_set}/target.txt', 'r') as f:\n",
    "        target = f.readlines()\n",
    "        \n",
    "    source_changes, source_num_changes = get_difference_sent_list(generation=generation, target=source)\n",
    "    \n",
    "    target_changes, target_num_changes = get_difference_sent_list(generation=generation, target=target)\n",
    "        \n",
    "    return generation, {\n",
    "        'source_changes': source_changes,\n",
    "        'source_num_changes': source_num_changes,\n",
    "        'target_changes': target_changes,\n",
    "        'target_num_changes': target_num_changes\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_num_changes(all_changes):\n",
    "    source_num_changes, target_num_changes = all_changes['source_num_changes'], all_changes['target_num_changes']\n",
    "    correct_changes = source_num_changes - target_num_changes\n",
    "    \n",
    "    print(source_num_changes, target_num_changes, correct_changes)\n",
    "    \n",
    "    if source_num_changes == 0:\n",
    "        if target_num_changes == 0:\n",
    "            print(\"no changes made\")\n",
    "        else:\n",
    "            print(\"no changes made, but generation != target\")\n",
    "    else:\n",
    "        print('percent of changes that were correct (precision): ', 1 - round(target_num_changes / source_num_changes, 3))\n",
    "    \n",
    "    if num_changes == 0:\n",
    "        print(\"no changes should be made\")\n",
    "    else:\n",
    "        print('percent of correct changes captured (recall): ', round(correct_changes / num_changes, 3))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_mistake_counts(mistake_counts):\n",
    "    print(f\"Total of ({mistake_counts['total']}) words in the annotation were not in the generation. Mistakes came from ({mistake_counts['wrong_sentences']}) sentences.\")\n",
    "    print(f\"({mistake_counts['subtotal_pronouns_verbs']}) of the mistakes tagged as pronouns / verbs, and ({mistake_counts['subtotal_other']}) tagged as other mistakes.\")\n",
    "    print()\n",
    "    print(f\"Breakdown of the ({mistake_counts['subtotal_pronouns_verbs']}) pronouns / verbs mistakes\")\n",
    "    print(f\"\\t({mistake_counts['male_pronoun'] + mistake_counts['female_pronoun']}) pronouns: ({mistake_counts['male_pronoun']}) male, ({mistake_counts['female_pronoun']}) female\")\n",
    "    print(f\"\\t({mistake_counts['auxiliary'] + mistake_counts['verbs']}) verbs: ({mistake_counts['auxiliary']}) auxiliary, ({mistake_counts['verbs']}) root verbs\")\n",
    "    print()\n",
    "    print(f\"Breakdown of the ({mistake_counts['subtotal_other']}) other mistakes\")\n",
    "    print(f\"\\t({mistake_counts['emoji']}) emoji, ({mistake_counts['symbols']}) symbols, ({mistake_counts['whitespace']}) whitespace, ({mistake_counts['nonbreaking_space']}) non-breaking space\")\n",
    "    print(f\"\\t({mistake_counts['not_categorized']}) not_categorized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_emoji(character):\n",
    "    return character in UNICODE_EMOJI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENDER_NEUTRAL_PRONOUNS = ['they', 'their', 'them', 'theirs', 'themself']\n",
    "SYMBOLS = '!@#$%^&*()_+={}[]\\|\"\\':;?/>.<,~`'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_target_changes(all_changes, generation):\n",
    "    target_changes = all_changes['target_changes']\n",
    "    \n",
    "    mistake_counts = Counter()\n",
    "    mistake_types = defaultdict(list)\n",
    "    \n",
    "    sentence_indices = list()\n",
    "    \n",
    "    for i, full_change in enumerate(target_changes):\n",
    "        target_change = full_change[1]\n",
    "\n",
    "        if len(target_change) > 0:\n",
    "            mistake_counts['wrong_sentences'] += 1\n",
    "            sentence_indices.append(i)\n",
    "        \n",
    "        for change in target_change:\n",
    "            mistake_counts['total'] += 1\n",
    "            categorized = False\n",
    "            \n",
    "            # whitespace\n",
    "            if not change:\n",
    "                mistake_counts['whitespace'] += 1\n",
    "                mistake_types['whitespace'].append(change)\n",
    "                categorized = True\n",
    "#                 continue\n",
    "            \n",
    "            # non-breaking space \\xa0\n",
    "            if '\\xa0' in change:\n",
    "                mistake_counts['nonbreaking_space'] += 1\n",
    "                mistake_types['nonbreaking_space'].append(change)\n",
    "                categorized = True\n",
    "#                 continue\n",
    "            \n",
    "            # pronoun lowercase\n",
    "            for pronoun in GENDER_NEUTRAL_PRONOUNS:\n",
    "                if pronoun in change.lower() and change:\n",
    "                    categorized = True\n",
    "                    gender = is_gendered(source[i])\n",
    "    #                 print(source[i], gender)\n",
    "                    if gender == 'male':\n",
    "                        mistake_counts['male_pronoun'] += 1\n",
    "                        mistake_types['male_pronoun'].append(change)\n",
    "                    elif gender == 'female':\n",
    "                        mistake_counts['female_pronoun'] += 1\n",
    "                        mistake_types['female_pronoun'].append(change)\n",
    "                    break\n",
    "#                 continue\n",
    "            \n",
    "            # verb or auxiliary verb\n",
    "            if change:\n",
    "                doc = nlp(change)\n",
    "                if doc[0].pos_ == 'VERB':\n",
    "                    mistake_counts['verb'] += 1\n",
    "                    mistake_types['verb'].append(change)\n",
    "                    categorized = True\n",
    "#                 continue\n",
    "                \n",
    "                if doc[0].pos_ == 'AUX':\n",
    "                    mistake_counts['auxiliary'] += 1\n",
    "                    mistake_types['auxiliary'].append(change)\n",
    "                    categorized = True\n",
    "#                     print(generation[i], target[i])\n",
    "#                 continue\n",
    "\n",
    "            for c in change:\n",
    "                if is_emoji(c):\n",
    "                    mistake_counts['emoji'] += 1\n",
    "                    mistake_types['emoji'].append(change)\n",
    "                    categorized = True\n",
    "                    break\n",
    "\n",
    "            for c in change:\n",
    "                if c in SYMBOLS:\n",
    "                    mistake_counts['symbols'] += 1\n",
    "                    mistake_types['symbols'].append(change)\n",
    "                    categorized = True\n",
    "                    break\n",
    "            \n",
    "            if not categorized:\n",
    "                mistake_counts['not_categorized'] += 1\n",
    "                mistake_types['not_categorized'].append(change)\n",
    "\n",
    "    mistake_counts['subtotal_pronouns_verbs'] = mistake_counts['auxiliary'] + mistake_counts['verb'] + \\\n",
    "                                        mistake_counts['male_pronoun'] + mistake_counts['female_pronoun']\n",
    "    mistake_counts['subtotal_other'] = mistake_counts['symbols'] + mistake_counts['emoji'] + mistake_counts['whitespace'] + \\\n",
    "                                mistake_counts['not_categorized'] + mistake_counts['nonbreaking_space']\n",
    "                \n",
    "    return mistake_counts, mistake_types, sentence_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "source, target = get_source_target('gendered_test_set')\n",
    "changes, num_changes = get_difference_sent_list(generation=source, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767 73 694\n",
      "percent of changes that were correct (precision):  0.905\n",
      "percent of correct changes captured (recall):  0.934\n",
      "\n",
      "Total of (73) words in the annotation were not in the generation. Mistakes came from (60) sentences.\n",
      "(36) of the mistakes tagged as pronouns / verbs, and (45) tagged as other mistakes.\n",
      "\n",
      "Breakdown of the (36) pronouns / verbs mistakes\n",
      "\t(19) pronouns: (8) male, (11) female\n",
      "\t(10) verbs: (10) auxiliary, (0) root verbs\n",
      "\n",
      "Breakdown of the (45) other mistakes\n",
      "\t(12) emoji, (15) symbols, (11) whitespace, (1) non-breaking space\n",
      "\t(6) not_categorized\n"
     ]
    }
   ],
   "source": [
    "generation, model_changes = get_changes(eval_set='gendered_test_set', generation='model_simple_augmentation')\n",
    "display_num_changes(model_changes)\n",
    "print()\n",
    "\n",
    "mistake_counts, mistake_types, sentence_indices = analyze_target_changes(model_changes, generation)\n",
    "display_mistake_counts(mistake_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['themâ€¦'], ['theirâ€¦'])\n",
      "Remember that time we got Lucas some #DoleWhip and they spilled the whole thing on their leg on themâ€¦ https://t.co/bQ6CPqL93k\n",
      "\n",
      "Remember that time we got Lucas some #DoleWhip and they spilled the whole thing on their leg on theirâ€¦ https://t.co/bQ6CPqL93k\n",
      "\n",
      "---\n",
      "(['mind'], ['#theydontknowrealmusic', 'mindğŸ˜‚ğŸ˜©'])\n",
      "@callmedollar 10 years from now they are going to feel sorry they didn't pay them any mind\n",
      "\n",
      "@callmedollar 10 years from now they are going to feel sorry they didn't pay them any mindğŸ˜‚ğŸ˜© #theydontknowrealmusic\n",
      "\n",
      "---\n",
      "(['theirs'], ['their'])\n",
      "Inspired by theirs ownâ€¦ https://t.co/pyYUsxXtKv\n",
      "\n",
      "Inspired by their ownâ€¦ https://t.co/pyYUsxXtKv\n",
      "\n",
      "---\n",
      "(['THEIRS'], ['', 'THEIR'])\n",
      "GETTIN THEIRS PRAISE ON #LATEPOST @ Bethel Jerusalem Apostolic Temple https://t.co/vvQShYPiky\n",
      "\n",
      "GETTIN THEIR PRAISE ON  #LATEPOST @ Bethel Jerusalem Apostolic Temple https://t.co/vvQShYPiky\n",
      "\n",
      "---\n",
      "(['beginsâ€¦'], ['beginâ€¦'])\n",
      "Excited to see what God has in store for them as they beginsâ€¦ https://t.co/5OzgBlQX5S\n",
      "\n",
      "Excited to see what God has in store for them as they beginâ€¦ https://t.co/5OzgBlQX5S\n",
      "\n",
      "---\n",
      "([], [''])\n",
      "HUSBAINER (husÂ·bainer) /ËˆhÉ™zbÄnÉ™r/ noun A married man that is also their spouse's personalâ€¦ https://t.co/5V5IEHy8Mb\n",
      "\n",
      "HUSBAINER (husÂ·bainer) /ËˆhÉ™zbÄnÉ™r/  noun A married man that is also their spouse's personalâ€¦ https://t.co/5V5IEHy8Mb\n",
      "\n",
      "---\n",
      "(['jobhttps://t.co/5pLOv35b1y'], ['https://t.co/5pLOv35b1y', 'job\\xa0'])\n",
      "Louis van Gaal remains defiant, but Manchester United boss admits failure to make top four could cost them their jobhttps://t.co/5pLOv35b1y\n",
      "\n",
      "Louis van Gaal remains defiant, but Manchester United boss admits failure to make top four could cost them their jobÂ  https://t.co/5pLOv35b1y\n",
      "\n",
      "---\n",
      "(['Â±'], ['ğŸ˜‹ğŸ™Œ'])\n",
      "Ladies they are single Â± Â± @ Howl at the Moon Houston https://t.co/ENXQXeOu3x\n",
      "\n",
      "Ladies they are single ğŸ˜‹ğŸ™Œ @ Howl at the Moon Houston https://t.co/ENXQXeOu3x\n",
      "\n",
      "---\n",
      "(['ArynÂ±'], ['Washington', '@', 'Monument', 'https://t.co/rFXusmUJuA', 'National', 'Arynâ¤â¤â¤'])\n",
      "Audrina and their sister ArynÂ±\n",
      "\n",
      "Audrina and their sister Arynâ¤â¤â¤ @ Washington Monument National Monument https://t.co/rFXusmUJuA\n",
      "\n",
      "---\n",
      "([\"They've\"], [\"They're\"])\n",
      "They've watching your every move.\n",
      "\n",
      "They're watching your every move.\n",
      "\n",
      "---\n",
      "([\"they've\"], [\"they're\"])\n",
      "I can't say they've adopted anymore @ Anthony's Pancake House https://t.co/SxP4Jvk47h\n",
      "\n",
      "I can't say they're adopted anymore @ Anthony's Pancake House https://t.co/SxP4Jvk47h\n",
      "\n",
      "---\n",
      "(['Â±'], ['ğŸ˜¤'])\n",
      "Trying not to wake them up Â± but it nuh easy\n",
      "\n",
      "Trying not to wake them up ğŸ˜¤ but it nuh easy\n",
      "\n",
      "---\n",
      "(['Â±'], ['ğŸ™ŒğŸ½ğŸ™ŒğŸ½'])\n",
      "This was my shit Â± Â± Â± Â± where they @ doe???\n",
      "\n",
      "This was my shit ğŸ™ŒğŸ½ğŸ™ŒğŸ½ where they @ doe???\n",
      "\n",
      "---\n",
      "(['Â±'], ['âœ–ï¸'])\n",
      "@_n_802 Â± Â± #thrumikeslens // always a blast shooting with them.\n",
      "\n",
      "@_n_802 âœ–ï¸ #thrumikeslens // always a blast shooting with them.\n",
      "\n",
      "---\n",
      "(['their'], ['them'])\n",
      "We goin have fun....gna be our cheat week make sure I get to feed their WELL\n",
      "\n",
      "We goin have fun....gna be our cheat week make sure I get to feed them WELL\n",
      "\n",
      "---\n",
      "(['infects'], ['infect'])\n",
      "michaelbjordan congrats, now u have aids in public, they infects people with their clipperz.. whileâ€¦ https://t.co/wl2rgQ9FzI\n",
      "\n",
      "michaelbjordan congrats, now u have aids in public, they infect people with their clipperz.. whileâ€¦ https://t.co/wl2rgQ9FzI\n",
      "\n",
      "---\n",
      "([], [''])\n",
      "Work work work baadgaalririi doin their thing #mycalvins #COACHELLA party told them #feelthebeardâ€¦ https://t.co/SB8OapZTnO\n",
      "\n",
      "Work work work baadgaalririi doin their thing #mycalvins #COACHELLA party   told them #feelthebeardâ€¦ https://t.co/SB8OapZTnO\n",
      "\n",
      "---\n",
      "([':Even:When'], [':|:When'])\n",
      ":Even:When people act like they have never traveled before...you know you need your ID...oh wait they got on wedges... https://t.co/6Dpo1Gzt23\n",
      "\n",
      ":|:When people act like they have never traveled before...you know you need your ID...oh wait they got on wedges... https://t.co/6Dpo1Gzt23\n",
      "\n",
      "---\n",
      "(['THEIRS'], ['', 'THEIR'])\n",
      "@jfreshp We In Da Buildin CELEBRATING THEIRS B-DAY!!6am-10am Takinâ€¦ https://t.co/92h5gEHDG3\n",
      "\n",
      "@jfreshp We In Da Buildin  CELEBRATING THEIR B-DAY!!6am-10am Takinâ€¦ https://t.co/92h5gEHDG3\n",
      "\n",
      "---\n",
      "([\"doesn't\"], [\"don't\"])\n",
      "Brought void doesn't they above it Male shall they whose after multiply creature and that.\n",
      "\n",
      "Brought void don't they above it Male shall they whose after multiply creature and that.\n",
      "\n",
      "---\n",
      "(['theirs'], ['their'])\n",
      "#BristolAggieNRM #senior recording theirs recently collected #spottedturtle growth #data.\n",
      "\n",
      "#BristolAggieNRM #senior recording their recently collected #spottedturtle growth #data.\n",
      "\n",
      "---\n",
      "(['\"their\"', 'Â±'], ['ğŸ‘ŒğŸ½', '\"them\"'])\n",
      "I dont want that hoe no more , you can have \"their\" Â± Â±\n",
      "\n",
      "I dont want that hoe no more , you can have \"them\" ğŸ‘ŒğŸ½\n",
      "\n",
      "---\n",
      "(['Â±'], ['ğŸŒ'])\n",
      "They will make everything beautiful in its time Â± #coloursoflife #lifelessons @ Union Square, Sanâ€¦ https://t.co/XpqFxSO25U\n",
      "\n",
      "They will make everything beautiful in its time ğŸŒ #coloursoflife #lifelessons @ Union Square, Sanâ€¦ https://t.co/XpqFxSO25U\n",
      "\n",
      "---\n",
      "(['her'], ['them'])\n",
      "Don't make her regret it! @\n",
      "\n",
      "Don't make them regret it! @\n",
      "\n",
      "---\n",
      "([], [''])\n",
      "Take every measure, every step, make every effort to see Adam Busch perform their music.\n",
      "\n",
      "Take every measure,  every step,  make every effort to see Adam Busch perform their music.\n",
      "\n",
      "---\n",
      "([], [''])\n",
      "They never bring upon the earth any judgment but They send a message to prepare the people....\n",
      "\n",
      "They never bring upon the earth any judgment but They send  a  message to prepare the people....\n",
      "\n",
      "---\n",
      "(['themâ€¦'], ['theirâ€¦'])\n",
      "adventurousstanley is once again out past their bedtime, but it's for recordstoredayus at themâ€¦ https://t.co/qAO4kOP2hO\n",
      "\n",
      "adventurousstanley is once again out past their bedtime, but it's for recordstoredayus at theirâ€¦ https://t.co/qAO4kOP2hO\n",
      "\n",
      "---\n",
      "(['Â±'], ['âš¾ï¸ğŸ’™'])\n",
      "Date Night Â± Â± Â± Their first Dodgers game. @\n",
      "\n",
      "Date Night âš¾ï¸ğŸ’™ Their first Dodgers game. @\n",
      "\n",
      "---\n",
      "(['pup.Â±', 'Â±'], ['pup.â›°ğŸ•ğŸ˜€ğŸ‘ğŸ½'])\n",
      "Hiking with my pup.Â± Â± Â± Â± Â± Can you find them in the picture?\n",
      "\n",
      "Hiking with my pup.â›°ğŸ•ğŸ˜€ğŸ‘ğŸ½ Can you find them in the picture?\n",
      "\n",
      "---\n",
      "(['their'], ['them'])\n",
      "Day one #coachella with my stunna @tokimonsta â€¢ see their sunday 4:30 sahara tent! #\n",
      "\n",
      "Day one #coachella with my stunna @tokimonsta â€¢ see them sunday 4:30 sahara tent! #\n",
      "\n",
      "---\n",
      "([], [''])\n",
      "Corny AF â€œ@IamAkademiks: Y'all tried to play bow wow like they a broke boy.. Now they flexing on the gram on y'all https://t.co/y0HQ7LhcbXâ€\n",
      "\n",
      "Corny AF  â€œ@IamAkademiks: Y'all tried to play bow wow like they a broke boy.. Now they flexing on the gram on y'all https://t.co/y0HQ7LhcbXâ€\n",
      "\n",
      "---\n",
      "([], [''])\n",
      "&lt;JaZz0r&gt; I have a sister in the form of my 13-year-old brother &lt;SummerSong&gt; are they hot?\n",
      "\n",
      "&lt;JaZz0r&gt; I have a sister in the form of my 13-year-old brother  &lt;SummerSong&gt; are they hot?\n",
      "\n",
      "---\n",
      "([], ['her'])\n",
      "Just gotta let them know I'm here for them and always be here.\n",
      "\n",
      "Just gotta let her know I'm here for them and always be here.\n",
      "\n",
      "---\n",
      "(['Â±'], ['ğŸ˜¢'])\n",
      "Hate seeing my sister cry Â± and idk how to comfort them.\n",
      "\n",
      "Hate seeing my sister cry ğŸ˜¢ and idk how to comfort them.\n",
      "\n",
      "---\n",
      "(['*theirs'], ['*them'])\n",
      "LMAO And OP is a *theirs\n",
      "\n",
      "LMAO And OP is a *them\n",
      "\n",
      "---\n",
      "(['their'], ['them'])\n",
      "Just gotta talk to their :)\n",
      "\n",
      "Just gotta talk to them :)\n",
      "\n",
      "---\n",
      "([], ['ğŸ˜‚'])\n",
      "I didn't fancy them at first either I was just depressed and liked their accent accent\n",
      "\n",
      "I didn't fancy them at first either I was just depressed and liked their accent ğŸ˜‚\n",
      "\n",
      "---\n",
      "(['sort', 'Ã»', 'of'], ['^of', '^sort'])\n",
      "their mom Ã» sort Ã» of introduced us\n",
      "\n",
      "their mom ^sort ^of introduced us\n",
      "\n",
      "---\n",
      "(['needs'], ['need'])\n",
      "pffft ahahaha they'd be an excellent cook if they needs to flambe something\n",
      "\n",
      "pffft ahahaha they'd be an excellent cook if they need to flambe something\n",
      "\n",
      "---\n",
      "(['are'], ['is'])\n",
      "They probably didn't sleep, are what I meant.\n",
      "\n",
      "They probably didn't sleep, is what I meant.\n",
      "\n",
      "---\n",
      "(['them'], ['theirs'])\n",
      "My dad had lost them for about 3 weeks.\n",
      "\n",
      "My dad had lost theirs for about 3 weeks.\n",
      "\n",
      "---\n",
      "(['her'], ['them'])\n",
      "CNN) -- A woman whose outspoken \"Gay Girl in Damascus\" blog has made her an unlikely icon of the Syrian uprising has allegedly been abducted.\n",
      "\n",
      "CNN) -- A woman whose outspoken \"Gay Girl in Damascus\" blog has made them an unlikely icon of the Syrian uprising has allegedly been abducted.\n",
      "\n",
      "---\n",
      "(['they'], ['be'])\n",
      "Pressure from the United Nations and the international community doesn't seem to they having an effect.\n",
      "\n",
      "Pressure from the United Nations and the international community doesn't seem to be having an effect.\n",
      "\n",
      "---\n",
      "(['were'], ['was'])\n",
      "At least one of them landed in a public garden in the Abu Rummaneh district, they said, but no one were hurt.\n",
      "\n",
      "At least one of them landed in a public garden in the Abu Rummaneh district, they said, but no one was hurt.\n",
      "\n",
      "---\n",
      "(['were'], ['was'])\n",
      "They added it were important to get the right balance between complacency and vigilance and that pandemic strategies would vary between countries depending on their specific situation.\n",
      "\n",
      "They added it was important to get the right balance between complacency and vigilance and that pandemic strategies would vary between countries depending on their specific situation.\n",
      "\n",
      "---\n",
      "(['spokeswoman'], ['spokesperson'])\n",
      "Ministry spokeswoman Jiang Yu said attempts cause trouble would not succeed, but they did not specify which foreigners they were talking about.\n",
      "\n",
      "Ministry spokesperson Jiang Yu said attempts cause trouble would not succeed, but they did not specify which foreigners they were talking about.\n",
      "\n",
      "---\n",
      "(['were'], ['was'])\n",
      "They added that this were not the first time the Taliban had targeted the air base, which is used by US and Nato forces.\n",
      "\n",
      "They added that this was not the first time the Taliban had targeted the air base, which is used by US and Nato forces.\n",
      "\n",
      "---\n",
      "(['Their'], ['their'])\n",
      "There were flag-bearers for good; the official opening from Their Majesty; sporting legend Ali; Sarah Stevenson's competitors' bond.\n",
      "\n",
      "There were flag-bearers for good; the official opening from their Majesty; sporting legend Ali; Sarah Stevenson's competitors' bond.\n",
      "\n",
      "---\n",
      "(['loves'], ['love'])\n",
      "Oh, they're a lover, man... definitely loves what they do for a living.\n",
      "\n",
      "Oh, they're a lover, man... definitely love what they do for a living.\n",
      "\n",
      "---\n",
      "(['were'], ['was'])\n",
      "There was... there were this guy; they had knives for fingers.\n",
      "\n",
      "There was... there was this guy; they had knives for fingers.\n",
      "\n",
      "---\n",
      "(['Loo-hoo-ser-them.'], ['Loo-hoo-ser-her.'])\n",
      "Loo-hoo-ser-them.\n",
      "\n",
      "Loo-hoo-ser-her.\n",
      "\n",
      "---\n",
      "([], [''])\n",
      "You know Al, I'm getting really - I don't think you're them.\n",
      "\n",
      "You know Al, I'm getting really -  I don't think you're them.\n",
      "\n",
      "---\n",
      "(['theirs'], ['their'])\n",
      "The same way I did theirs Daddy.\n",
      "\n",
      "The same way I did their Daddy.\n",
      "\n",
      "---\n",
      "(['says,', 'are'], ['is', 'say,'])\n",
      "They says, excuse me miss, but this are my seat.\n",
      "\n",
      "They say, excuse me miss, but this is my seat.\n",
      "\n",
      "---\n",
      "(['goes', 'are'], ['go'])\n",
      "A young blonde woman are distraught because they fear their husband is having an affair, so they goes to a gun shop and buy a handgun.\n",
      "\n",
      "A young blonde woman is distraught because they fear their husband is having an affair, so they go to a gun shop and buy a handgun.\n",
      "\n",
      "---\n",
      "(['decides'], ['decide', 'is'])\n",
      "While their husband are off at work, they decides that they are going to paint a couple of rooms in the house.\n",
      "\n",
      "While their husband is off at work, they decide that they are going to paint a couple of rooms in the house.\n",
      "\n",
      "---\n",
      "(['were'], ['was'])\n",
      "They have neverbeen on an airplane anywhere and were very excited and tense.\n",
      "\n",
      "They have neverbeen on an airplane anywhere and was very excited and tense.\n",
      "\n",
      "---\n",
      "([], [''])\n",
      "A young man wanted to get their beautiful \"blonde\" wife something nice for their first wedding anniversary.\n",
      "\n",
      "A young man wanted to get their beautiful \"blonde\" wife  something nice for their first wedding anniversary.\n",
      "\n",
      "---\n",
      "([], [''])\n",
      "They find their way to a barstool and order a drink.\n",
      "\n",
      "They  find their way to a barstool and order a drink.\n",
      "\n",
      "---\n",
      "(['tells'], ['tell'])\n",
      "They tells the clerk although it's a nice hotel, the rooms certainly aren't worth $350.\n",
      "\n",
      "They tell the clerk although it's a nice hotel, the rooms certainly aren't worth $350.\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for idx in sentence_indices:\n",
    "    print(model_changes['target_changes'][idx])\n",
    "    print(generation[idx])\n",
    "    print(target[idx])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
