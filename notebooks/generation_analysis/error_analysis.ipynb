{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import spacy\n",
    "from emoji import UNICODE_EMOJI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 16.7 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sys\n",
    "sys.path.append('../../neutral_generation/')\n",
    "from is_gendered import is_gendered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('male', 'female')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_gendered('he'), is_gendered('she')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_target(eval_set):\n",
    "    with open(f'../../evaluation/{eval_set}/source.txt', 'r') as f:\n",
    "        source = f.readlines()\n",
    "    \n",
    "    with open(f'../../evaluation/{eval_set}/target.txt', 'r') as f:\n",
    "        target = f.readlines()\n",
    "        \n",
    "    return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "source, target = get_source_target('gendered_test_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 500)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(source), len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter = Counter()\n",
    "\n",
    "for sent in source:\n",
    "    tokens = sent.lower().split(' ')\n",
    "    for token in tokens:\n",
    "        token_counter[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2522"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num of unique tokens\n",
    "len(token_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6427"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens = 0\n",
    "for token in token_counter:\n",
    "    total_tokens += token_counter[token]\n",
    "total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 274),\n",
       " ('a', 180),\n",
       " ('to', 158),\n",
       " ('he', 146),\n",
       " ('she', 136),\n",
       " ('and', 131),\n",
       " ('her', 122),\n",
       " ('in', 97),\n",
       " ('his', 95),\n",
       " ('was', 93),\n",
       " ('i', 88),\n",
       " ('of', 82),\n",
       " ('for', 63),\n",
       " ('is', 57),\n",
       " ('on', 50),\n",
       " ('that', 47),\n",
       " ('my', 46),\n",
       " ('with', 46),\n",
       " ('has', 38),\n",
       " ('you', 38)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def get_difference(old_sent: str, new_sent: str):\n",
    "    old_words = old_sent.strip().split(' ')\n",
    "    new_words = new_sent.strip().split(' ')\n",
    "    \n",
    "    removed_words = list(set(old_words) - set(new_words))\n",
    "    added_words = list(set(new_words) - set(old_words))\n",
    "#     removed_words = list(word for i, word in enumerate(old_words) if word != new_words[i])\n",
    "#     added_words = list(word for i, word in enumerate(new_words) if word != old_words[i])\n",
    "    \n",
    "    return removed_words, added_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_difference_sent_list(generation, target):\n",
    "    changes = list()\n",
    "    num_changes = 0\n",
    "\n",
    "    for i in range(500):\n",
    "        diffs = get_difference(generation[i], target[i])\n",
    "    #     if len(diffs[0]) != len(diffs[1]):\n",
    "    #         print(diffs)\n",
    "    #         print(generation[i], target[i])\n",
    "\n",
    "        changes.append(diffs)\n",
    "        num_changes = num_changes + len(diffs[1])\n",
    "    \n",
    "    return changes, num_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "743"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1590 / 795 / 795 for perfect count\n",
    "changes, num_changes = get_difference_sent_list(generation=source, target=target)\n",
    "num_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximation, isn't perfect\n",
    "# e.g. set subtraction for word diff, \n",
    "# e.g. calculation of percent of \"correct changes\" bc target changes aren't necessarily in source changes\n",
    "\n",
    "def get_changes(eval_set, generation):\n",
    "    with open(f'../../evaluation/{eval_set}/generations/{generation}/generation.txt', 'r') as f:\n",
    "        generation = f.readlines()\n",
    "    \n",
    "    with open(f'../../evaluation/{eval_set}/source.txt', 'r') as f:\n",
    "        source = f.readlines()\n",
    "    \n",
    "    with open(f'../../evaluation/{eval_set}/target.txt', 'r') as f:\n",
    "        target = f.readlines()\n",
    "        \n",
    "    source_changes, source_num_changes = get_difference_sent_list(generation=generation, target=source)\n",
    "    \n",
    "    target_changes, target_num_changes = get_difference_sent_list(generation=generation, target=target)\n",
    "        \n",
    "    return generation, {\n",
    "        'source_changes': source_changes,\n",
    "        'source_num_changes': source_num_changes,\n",
    "        'target_changes': target_changes,\n",
    "        'target_num_changes': target_num_changes\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_num_changes(all_changes):\n",
    "    source_num_changes, target_num_changes = all_changes['source_num_changes'], all_changes['target_num_changes']\n",
    "    correct_changes = source_num_changes - target_num_changes\n",
    "    \n",
    "    print(source_num_changes, target_num_changes, correct_changes)\n",
    "    \n",
    "    if source_num_changes == 0:\n",
    "        if target_num_changes == 0:\n",
    "            print(\"no changes made\")\n",
    "        else:\n",
    "            print(\"no changes made, but generation != target\")\n",
    "    else:\n",
    "        print('percent of changes that were correct (precision): ', 1 - round(target_num_changes / source_num_changes, 3))\n",
    "    \n",
    "    if num_changes == 0:\n",
    "        print(\"no changes should be made\")\n",
    "    else:\n",
    "        print('percent of correct changes captured (recall): ', round(correct_changes / num_changes, 3))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_mistake_counts(mistake_counts):\n",
    "    print(f\"Total of ({mistake_counts['total']}) words in the annotation were not in the generation. Mistakes came from ({mistake_counts['wrong_sentences']}) sentences.\")\n",
    "    print(f\"({mistake_counts['subtotal_pronouns_verbs']}) of the mistakes tagged as pronouns / verbs, and ({mistake_counts['subtotal_other']}) tagged as other mistakes.\")\n",
    "    print()\n",
    "    print(f\"Breakdown of the ({mistake_counts['subtotal_pronouns_verbs']}) pronouns / verbs mistakes\")\n",
    "    print(f\"\\t({mistake_counts['male_pronoun'] + mistake_counts['female_pronoun']}) pronouns: ({mistake_counts['male_pronoun']}) male, ({mistake_counts['female_pronoun']}) female\")\n",
    "    print(f\"\\t({mistake_counts['auxiliary'] + mistake_counts['verbs']}) verbs: ({mistake_counts['auxiliary']}) auxiliary, ({mistake_counts['verb']}) root verbs\")\n",
    "    print()\n",
    "    print(f\"Breakdown of the ({mistake_counts['subtotal_other']}) other mistakes\")\n",
    "    print(f\"\\t({mistake_counts['emoji']}) emoji, ({mistake_counts['symbols']}) symbols, ({mistake_counts['whitespace']}) whitespace, ({mistake_counts['nonbreaking_space']}) non-breaking space\")\n",
    "    print(f\"\\t({mistake_counts['not_categorized']}) not_categorized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_emoji(character):\n",
    "    return character in UNICODE_EMOJI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENDER_NEUTRAL_PRONOUNS = ['they', 'their', 'them', 'theirs', 'themself']\n",
    "SYMBOLS = '!@#$%^&*()_+={}[]\\|\"\\':;?/>.<,~`'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_target_changes(all_changes, generation):\n",
    "    target_changes = all_changes['target_changes']\n",
    "    \n",
    "    mistake_counts = Counter()\n",
    "    mistake_types = defaultdict(list)\n",
    "    \n",
    "    sentence_indices = list()\n",
    "    \n",
    "    for i, full_change in enumerate(target_changes):\n",
    "        target_change = full_change[1]\n",
    "\n",
    "        if len(target_change) > 0:\n",
    "            mistake_counts['wrong_sentences'] += 1\n",
    "            sentence_indices.append(i)\n",
    "        \n",
    "        for change in target_change:\n",
    "            mistake_counts['total'] += 1\n",
    "            categorized = False\n",
    "            \n",
    "            # whitespace\n",
    "            if not change:\n",
    "                mistake_counts['whitespace'] += 1\n",
    "                mistake_types['whitespace'].append(change)\n",
    "                categorized = True\n",
    "#                 continue\n",
    "            \n",
    "            # non-breaking space \\xa0\n",
    "            if '\\xa0' in change:\n",
    "                mistake_counts['nonbreaking_space'] += 1\n",
    "                mistake_types['nonbreaking_space'].append(change)\n",
    "                categorized = True\n",
    "#                 continue\n",
    "            \n",
    "            # pronoun lowercase\n",
    "            for pronoun in GENDER_NEUTRAL_PRONOUNS:\n",
    "                if pronoun in change.lower() and change:\n",
    "                    categorized = True\n",
    "                    gender = is_gendered(source[i])\n",
    "    #                 print(source[i], gender)\n",
    "                    if gender == 'male':\n",
    "                        mistake_counts['male_pronoun'] += 1\n",
    "                        mistake_types['male_pronoun'].append(change)\n",
    "                    elif gender == 'female':\n",
    "                        mistake_counts['female_pronoun'] += 1\n",
    "                        mistake_types['female_pronoun'].append(change)\n",
    "                    break\n",
    "#                 continue\n",
    "            \n",
    "            # verb or auxiliary verb\n",
    "            if change:\n",
    "                doc = nlp(change)\n",
    "                if doc[0].pos_ == 'VERB':\n",
    "                    mistake_counts['verb'] += 1\n",
    "                    mistake_types['verb'].append(change)\n",
    "                    categorized = True\n",
    "#                 continue\n",
    "                \n",
    "                if doc[0].pos_ == 'AUX':\n",
    "                    mistake_counts['auxiliary'] += 1\n",
    "                    mistake_types['auxiliary'].append(change)\n",
    "                    categorized = True\n",
    "#                     print(generation[i], target[i])\n",
    "#                 continue\n",
    "\n",
    "            for c in change:\n",
    "                if is_emoji(c):\n",
    "                    mistake_counts['emoji'] += 1\n",
    "                    mistake_types['emoji'].append(change)\n",
    "                    categorized = True\n",
    "                    break\n",
    "\n",
    "            for c in change:\n",
    "                if c in SYMBOLS:\n",
    "                    mistake_counts['symbols'] += 1\n",
    "                    mistake_types['symbols'].append(change)\n",
    "                    categorized = True\n",
    "                    break\n",
    "            \n",
    "            if not categorized:\n",
    "                mistake_counts['not_categorized'] += 1\n",
    "                mistake_types['not_categorized'].append(change)\n",
    "\n",
    "    mistake_counts['subtotal_pronouns_verbs'] = mistake_counts['auxiliary'] + mistake_counts['verb'] + \\\n",
    "                                        mistake_counts['male_pronoun'] + mistake_counts['female_pronoun']\n",
    "    mistake_counts['subtotal_other'] = mistake_counts['symbols'] + mistake_counts['emoji'] + mistake_counts['whitespace'] + \\\n",
    "                                mistake_counts['not_categorized'] + mistake_counts['nonbreaking_space']\n",
    "                \n",
    "    return mistake_counts, mistake_types, sentence_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "source, target = get_source_target('gendered_test_set')\n",
    "changes, num_changes = get_difference_sent_list(generation=source, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767 73 694\n",
      "percent of changes that were correct (precision):  0.905\n",
      "percent of correct changes captured (recall):  0.934\n",
      "\n",
      "Total of (73) words in the annotation were not in the generation. Mistakes came from (60) sentences.\n",
      "(34) of the mistakes tagged as pronouns / verbs, and (45) tagged as other mistakes.\n",
      "\n",
      "Breakdown of the (34) pronouns / verbs mistakes\n",
      "\t(16) pronouns: (4) male, (12) female\n",
      "\t(9) verbs: (9) auxiliary, (9) root verbs\n",
      "\n",
      "Breakdown of the (45) other mistakes\n",
      "\t(12) emoji, (14) symbols, (11) whitespace, (1) non-breaking space\n",
      "\t(7) not_categorized\n"
     ]
    }
   ],
   "source": [
    "generation, model_changes = get_changes(eval_set='gendered_test_set', generation='model_sa_nt_10_3')\n",
    "display_num_changes(model_changes)\n",
    "print()\n",
    "\n",
    "mistake_counts, mistake_types, sentence_indices = analyze_target_changes(model_changes, generation)\n",
    "display_mistake_counts(mistake_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'male_pronoun': ['their',\n",
       "              \"they're\",\n",
       "              'THEIR',\n",
       "              'their',\n",
       "              'mountains?\"They'],\n",
       "             'symbols': [\"they're\",\n",
       "              \"They're\",\n",
       "              '*them',\n",
       "              'was.',\n",
       "              'say,',\n",
       "              'that?\"They',\n",
       "              'say:',\n",
       "              'say,',\n",
       "              'mountains?\"They'],\n",
       "             'female_pronoun': ['their',\n",
       "              'them',\n",
       "              'them',\n",
       "              \"They're\",\n",
       "              'them',\n",
       "              '*them',\n",
       "              'them',\n",
       "              'them',\n",
       "              'their',\n",
       "              'them',\n",
       "              'that?\"They'],\n",
       "             'auxiliary': ['be',\n",
       "              'was',\n",
       "              'was',\n",
       "              'was.',\n",
       "              'were',\n",
       "              'was',\n",
       "              'is',\n",
       "              'is',\n",
       "              'have'],\n",
       "             'not_categorized': ['spokesperson', 'love'],\n",
       "             'verb': ['say,', 'go', 'look', 'decide', 'say:', 'tell', 'say,']})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistake_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['mindä¸á»£#theydontknowrealmusic'], ['#theydontknowrealmusic', 'mindðð©'])\n",
      "generation:  @callmedollar 10 years from now they are going to feel sorry they didn't pay them any mindä¸á»£#theydontknowrealmusic\n",
      "\n",
      "annotation:  @callmedollar 10 years from now they are going to feel sorry they didn't pay them any mindðð© #theydontknowrealmusic\n",
      "\n",
      "---\n",
      "(['theirs'], ['their'])\n",
      "generation:  Inspired by theirs ownâ¦ https://t.co/pyYUsxXtKv\n",
      "\n",
      "annotation:  Inspired by their ownâ¦ https://t.co/pyYUsxXtKv\n",
      "\n",
      "---\n",
      "(['THEIRS'], ['', 'THEIR'])\n",
      "generation:  GETTIN THEIRS PRAISE ON #LATEPOST @ Bethel Jerusalem Apostolic Temple https://t.co/vvQShYPiky\n",
      "\n",
      "annotation:  GETTIN THEIR PRAISE ON  #LATEPOST @ Bethel Jerusalem Apostolic Temple https://t.co/vvQShYPiky\n",
      "\n",
      "---\n",
      "(['beginsâ¦'], ['beginâ¦'])\n",
      "generation:  Excited to see what God has in store for them as they beginsâ¦ https://t.co/5OzgBlQX5S\n",
      "\n",
      "annotation:  Excited to see what God has in store for them as they beginâ¦ https://t.co/5OzgBlQX5S\n",
      "\n",
      "---\n",
      "([], [''])\n",
      "generation:  HUSBAINER (husÂ·bainer) /ËhÉzbÄnÉr/ noun A married man that is also their spouse's personalâ¦ https://t.co/5V5IEHy8Mb\n",
      "\n",
      "annotation:  HUSBAINER (husÂ·bainer) /ËhÉzbÄnÉr/  noun A married man that is also their spouse's personalâ¦ https://t.co/5V5IEHy8Mb\n",
      "\n",
      "---\n",
      "(['jobhttps://t.co/5pLOv35b1y'], ['https://t.co/5pLOv35b1y', 'job\\xa0'])\n",
      "generation:  Louis van Gaal remains defiant, but Manchester United boss admits failure to make top four could cost them their jobhttps://t.co/5pLOv35b1y\n",
      "\n",
      "annotation:  Louis van Gaal remains defiant, but Manchester United boss admits failure to make top four could cost them their jobÂ  https://t.co/5pLOv35b1y\n",
      "\n",
      "---\n",
      "(['á»£Philad@'], ['ðð', '@'])\n",
      "generation:  Ladies they are single á»£Philad@ Howl at the Moon Houston https://t.co/ENXQXeOu3x\n",
      "\n",
      "annotation:  Ladies they are single ðð @ Howl at the Moon Houston https://t.co/ENXQXeOu3x\n",
      "\n",
      "---\n",
      "(['Arynä¸ä¸Philad@'], ['@', 'Arynâ¤â¤â¤'])\n",
      "generation:  Audrina and their sister Arynä¸ä¸Philad@ Washington Monument National Monument https://t.co/rFXusmUJuA\n",
      "\n",
      "annotation:  Audrina and their sister Arynâ¤â¤â¤ @ Washington Monument National Monument https://t.co/rFXusmUJuA\n",
      "\n",
      "---\n",
      "([\"they've\"], [\"they're\"])\n",
      "generation:  I can't say they've adopted anymore @ Anthony's Pancake House https://t.co/SxP4Jvk47h\n",
      "\n",
      "annotation:  I can't say they're adopted anymore @ Anthony's Pancake House https://t.co/SxP4Jvk47h\n",
      "\n",
      "---\n",
      "(['á»£but'], ['ð¤', 'but'])\n",
      "generation:  Trying not to wake them up á»£but it nuh easy\n",
      "\n",
      "annotation:  Trying not to wake them up ð¤ but it nuh easy\n",
      "\n",
      "---\n",
      "(['PhiladPhiladPhiladá»£where'], ['where', 'ðð½ðð½'])\n",
      "generation:  This was my shit PhiladPhiladPhiladá»£where they @ doe???\n",
      "\n",
      "annotation:  This was my shit ðð½ðð½ where they @ doe???\n",
      "\n",
      "---\n",
      "(['PhiladPhilad#thrumikeslens'], ['#thrumikeslens', 'âï¸'])\n",
      "generation:  @_n_802 PhiladPhilad#thrumikeslens // always a blast shooting with them.\n",
      "\n",
      "annotation:  @_n_802 âï¸ #thrumikeslens // always a blast shooting with them.\n",
      "\n",
      "---\n",
      "(['their'], ['them'])\n",
      "generation:  We goin have fun....gna be our cheat week make sure I get to feed their WELL\n",
      "\n",
      "annotation:  We goin have fun....gna be our cheat week make sure I get to feed them WELL\n",
      "\n",
      "---\n",
      "([], [''])\n",
      "generation:  Work work work baadgaalririi doin their thing #mycalvins #COACHELLA party told them #feelthebeardâ¦ https://t.co/SB8OapZTnO\n",
      "\n",
      "annotation:  Work work work baadgaalririi doin their thing #mycalvins #COACHELLA party   told them #feelthebeardâ¦ https://t.co/SB8OapZTnO\n",
      "\n",
      "---\n",
      "([':ä¸:When'], [':|:When'])\n",
      "generation:  :ä¸:When people act like they have never traveled before...you know you need your ID...oh wait they got on wedges... https://t.co/6Dpo1Gzt23\n",
      "\n",
      "annotation:  :|:When people act like they have never traveled before...you know you need your ID...oh wait they got on wedges... https://t.co/6Dpo1Gzt23\n",
      "\n",
      "---\n",
      "(['THEIRS'], ['', 'THEIR'])\n",
      "generation:  @jfreshp We In Da Buildin CELEBRATING THEIRS B-DAY!!6am-10am Takinâ¦ https://t.co/92h5gEHDG3\n",
      "\n",
      "annotation:  @jfreshp We In Da Buildin  CELEBRATING THEIR B-DAY!!6am-10am Takinâ¦ https://t.co/92h5gEHDG3\n",
      "\n",
      "---\n",
      "([\"doesn't\"], [\"don't\"])\n",
      "generation:  Brought void doesn't they above it Male shall they whose after multiply creature and that.\n",
      "\n",
      "annotation:  Brought void don't they above it Male shall they whose after multiply creature and that.\n",
      "\n",
      "---\n",
      "(['them'], ['their'])\n",
      "generation:  #BristolAggieNRM #senior recording them recently collected #spottedturtle growth #data.\n",
      "\n",
      "annotation:  #BristolAggieNRM #senior recording their recently collected #spottedturtle growth #data.\n",
      "\n",
      "---\n",
      "(['á»£'], ['ðð½'])\n",
      "generation:  I dont want that hoe no more , you can have \"them\" want á»£\n",
      "\n",
      "annotation:  I dont want that hoe no more , you can have \"them\" ðð½\n",
      "\n",
      "---\n",
      "(['á»£#coloursoflife'], ['ð', '#coloursoflife'])\n",
      "generation:  They will make everything beautiful in its time á»£#coloursoflife #lifelessons @ Union Square, Sanâ¦ https://t.co/XpqFxSO25U\n",
      "\n",
      "annotation:  They will make everything beautiful in its time ð #coloursoflife #lifelessons @ Union Square, Sanâ¦ https://t.co/XpqFxSO25U\n",
      "\n",
      "---\n",
      "(['their'], ['them'])\n",
      "generation:  Don't make their regret it! @\n",
      "\n",
      "annotation:  Don't make them regret it! @\n",
      "\n",
      "---\n",
      "([], [''])\n",
      "generation:  Take every measure, every step, make every effort to see Adam Busch perform their music.\n",
      "\n",
      "annotation:  Take every measure,  every step,  make every effort to see Adam Busch perform their music.\n",
      "\n",
      "---\n",
      "([], [''])\n",
      "generation:  They never bring upon the earth any judgment but They send a message to prepare the people....\n",
      "\n",
      "annotation:  They never bring upon the earth any judgment but They send  a  message to prepare the people....\n",
      "\n",
      "---\n",
      "(['á»£æPhiladTheir'], ['â¾ï¸ð', 'Their'])\n",
      "generation:  Date Night á»£æPhiladTheir first Dodgers game. @\n",
      "\n",
      "annotation:  Date Night â¾ï¸ð Their first Dodgers game. @\n",
      "\n",
      "---\n",
      "(['pup.á»£á»£á»£PhiladPhiladCan'], ['Can', 'pup.â°ðððð½'])\n",
      "generation:  Hiking with my pup.á»£á»£á»£PhiladPhiladCan you find them in the picture?\n",
      "\n",
      "annotation:  Hiking with my pup.â°ðððð½ Can you find them in the picture?\n",
      "\n",
      "---\n",
      "(['her'], ['them'])\n",
      "generation:  Day one #coachella with my stunna @tokimonsta â¢ see her sunday 4:30 sahara tent! #\n",
      "\n",
      "annotation:  Day one #coachella with my stunna @tokimonsta â¢ see them sunday 4:30 sahara tent! #\n",
      "\n",
      "---\n",
      "([\"doesn't\"], [\"don't\"])\n",
      "generation:  Behold you're, under green it creature living they'd for doesn't fly behold won't wherein yielding.\n",
      "\n",
      "annotation:  Behold you're, under green it creature living they'd for don't fly behold won't wherein yielding.\n",
      "\n",
      "---\n",
      "([], [''])\n",
      "generation:  Corny AF â@IamAkademiks: Y'all tried to play bow wow like they a broke boy.. Now they flexing on the gram on y'all https://t.co/y0HQ7LhcbXâ\n",
      "\n",
      "annotation:  Corny AF  â@IamAkademiks: Y'all tried to play bow wow like they a broke boy.. Now they flexing on the gram on y'all https://t.co/y0HQ7LhcbXâ\n",
      "\n",
      "---\n",
      "([], [''])\n",
      "generation:  &lt;JaZz0r&gt; I have a sister in the form of my 13-year-old brother &lt;SummerSong&gt; are they hot?\n",
      "\n",
      "annotation:  &lt;JaZz0r&gt; I have a sister in the form of my 13-year-old brother  &lt;SummerSong&gt; are they hot?\n",
      "\n",
      "---\n",
      "(['Philadand'], ['ð¢', 'and'])\n",
      "generation:  Hate seeing my sister cry Philadand idk how to comfort them.\n",
      "\n",
      "annotation:  Hate seeing my sister cry ð¢ and idk how to comfort them.\n",
      "\n",
      "---\n",
      "(['were'], ['was'])\n",
      "generation:  In a bar, had been stood up by my online date and they bought me a drink, that were 3 years ago :)\n",
      "\n",
      "annotation:  In a bar, had been stood up by my online date and they bought me a drink, that was 3 years ago :)\n",
      "\n",
      "---\n",
      "(['á»£'], ['ð'])\n",
      "generation:  I didn't fancy them at first either I was just depressed and liked their accent á»£\n",
      "\n",
      "annotation:  I didn't fancy them at first either I was just depressed and liked their accent ð\n",
      "\n",
      "---\n",
      "(['are'], ['is'])\n",
      "generation:  They probably didn't sleep, are what I meant.\n",
      "\n",
      "annotation:  They probably didn't sleep, is what I meant.\n",
      "\n",
      "---\n",
      "(['her'], ['them'])\n",
      "generation:  I was completely expecting you to make her cry the bad way, so that was a wholesome twist\n",
      "\n",
      "annotation:  I was completely expecting you to make them cry the bad way, so that was a wholesome twist\n",
      "\n",
      "---\n",
      "(['are,'], ['is,'])\n",
      "generation:  No their family are, they are not.\n",
      "\n",
      "annotation:  No their family is, they are not.\n",
      "\n",
      "---\n",
      "(['her'], ['them'])\n",
      "generation:  CNN) -- A woman whose outspoken \"Gay Girl in Damascus\" blog has made her an unlikely icon of the Syrian uprising has allegedly been abducted.\n",
      "\n",
      "annotation:  CNN) -- A woman whose outspoken \"Gay Girl in Damascus\" blog has made them an unlikely icon of the Syrian uprising has allegedly been abducted.\n",
      "\n",
      "---\n",
      "(['they'], ['be'])\n",
      "generation:  Pressure from the United Nations and the international community doesn't seem to they having an effect.\n",
      "\n",
      "annotation:  Pressure from the United Nations and the international community doesn't seem to be having an effect.\n",
      "\n",
      "---\n",
      "(['were'], ['was'])\n",
      "generation:  They added it were important to get the right balance between complacency and vigilance and that pandemic strategies would vary between countries depending on their specific situation.\n",
      "\n",
      "annotation:  They added it was important to get the right balance between complacency and vigilance and that pandemic strategies would vary between countries depending on their specific situation.\n",
      "\n",
      "---\n",
      "(['were'], ['was'])\n",
      "generation:  It were now a \"serious diplomatic incident\", they said, with the British authorities trying to secure their freedom.\n",
      "\n",
      "annotation:  It was now a \"serious diplomatic incident\", they said, with the British authorities trying to secure their freedom.\n",
      "\n",
      "---\n",
      "(['spokeswoman'], ['spokesperson'])\n",
      "generation:  Ministry spokeswoman Jiang Yu said attempts cause trouble would not succeed, but they did not specify which foreigners they were talking about.\n",
      "\n",
      "annotation:  Ministry spokesperson Jiang Yu said attempts cause trouble would not succeed, but they did not specify which foreigners they were talking about.\n",
      "\n",
      "---\n",
      "(['were'], ['was'])\n",
      "generation:  They added that this were not the first time the Taliban had targeted the air base, which is used by US and Nato forces.\n",
      "\n",
      "annotation:  They added that this was not the first time the Taliban had targeted the air base, which is used by US and Nato forces.\n",
      "\n",
      "---\n",
      "(['Their'], ['their'])\n",
      "generation:  There were flag-bearers for good; the official opening from Their Majesty; sporting legend Ali; Sarah Stevenson's competitors' bond.\n",
      "\n",
      "annotation:  There were flag-bearers for good; the official opening from their Majesty; sporting legend Ali; Sarah Stevenson's competitors' bond.\n",
      "\n",
      "---\n",
      "(['loves'], ['love'])\n",
      "generation:  Oh, they're a lover, man... definitely loves what they do for a living.\n",
      "\n",
      "annotation:  Oh, they're a lover, man... definitely love what they do for a living.\n",
      "\n",
      "---\n",
      "(['were...', 'were'], ['was...', 'was'])\n",
      "generation:  There were... there were this guy; they had knives for fingers.\n",
      "\n",
      "annotation:  There was... there was this guy; they had knives for fingers.\n",
      "\n",
      "---\n",
      "(['her'], ['them'])\n",
      "generation:  Why don't you shut up and let her talk?\n",
      "\n",
      "annotation:  Why don't you shut up and let them talk?\n",
      "\n",
      "---\n",
      "(['Loo-hoo-ser-them.'], ['Loo-hoo-ser-her.'])\n",
      "generation:  Loo-hoo-ser-them.\n",
      "\n",
      "annotation:  Loo-hoo-ser-her.\n",
      "\n",
      "---\n",
      "([], [''])\n",
      "generation:  You know Al, I'm getting really - I don't think you're them.\n",
      "\n",
      "annotation:  You know Al, I'm getting really -  I don't think you're them.\n",
      "\n",
      "---\n",
      "(['theirs'], ['their'])\n",
      "generation:  And you, I take it, are the Paderewski who plays theirs concerto on me, the piano?\n",
      "\n",
      "annotation:  And you, I take it, are the Paderewski who plays their concerto on me, the piano?\n",
      "\n",
      "---\n",
      "(['them'], ['their'])\n",
      "generation:  The same way I did them Daddy.\n",
      "\n",
      "annotation:  The same way I did their Daddy.\n",
      "\n",
      "---\n",
      "(['says,', 'are'], ['is', 'say,'])\n",
      "generation:  They says, excuse me miss, but this are my seat.\n",
      "\n",
      "annotation:  They say, excuse me miss, but this is my seat.\n",
      "\n",
      "---\n",
      "(['goes', 'are'], ['go'])\n",
      "generation:  A young blonde woman are distraught because they fear their husband is having an affair, so they goes to a gun shop and buy a handgun.\n",
      "\n",
      "annotation:  A young blonde woman is distraught because they fear their husband is having an affair, so they go to a gun shop and buy a handgun.\n",
      "\n",
      "---\n",
      "(['looks'], ['look'])\n",
      "generation:  They study the pieces for a moment, then looks at the box.\n",
      "\n",
      "annotation:  They study the pieces for a moment, then look at the box.\n",
      "\n",
      "---\n",
      "(['decides'], ['decide', 'is'])\n",
      "generation:  While their husband are off at work, they decides that they are going to paint a couple of rooms in the house.\n",
      "\n",
      "annotation:  While their husband is off at work, they decide that they are going to paint a couple of rooms in the house.\n",
      "\n",
      "---\n",
      "(['were'], ['was'])\n",
      "generation:  They have neverbeen on an airplane anywhere and were very excited and tense.\n",
      "\n",
      "annotation:  They have neverbeen on an airplane anywhere and was very excited and tense.\n",
      "\n",
      "---\n",
      "(['asks'], ['ask'])\n",
      "generation:  They then asks the clerk,\"Can I have that television set over there.\"\n",
      "\n",
      "annotation:  They then ask the clerk,\"Can I have that television set over there.\"\n",
      "\n",
      "---\n",
      "(['that?\"She'], ['that?\"They'])\n",
      "generation:  The other 2 ask \"How'd you do that?\"She say: \"Well, I followed some tracks,and BOOM, I got a rabbit.\"\n",
      "\n",
      "annotation:  The other 2 ask \"How'd you do that?\"They say: \"Well, I followed some tracks,and BOOM, I got a rabbit.\"\n",
      "\n",
      "---\n",
      "([], [''])\n",
      "generation:  A young man wanted to get their beautiful \"blonde\" wife something nice for their first wedding anniversary.\n",
      "\n",
      "annotation:  A young man wanted to get their beautiful \"blonde\" wife  something nice for their first wedding anniversary.\n",
      "\n",
      "---\n",
      "([], [''])\n",
      "generation:  They find their way to a barstool and order a drink.\n",
      "\n",
      "annotation:  They  find their way to a barstool and order a drink.\n",
      "\n",
      "---\n",
      "(['tells'], ['tell'])\n",
      "generation:  They tells the clerk although it's a nice hotel, the rooms certainly aren't worth $350.\n",
      "\n",
      "annotation:  They tell the clerk although it's a nice hotel, the rooms certainly aren't worth $350.\n",
      "\n",
      "---\n",
      "(['says,', 'mountains?\"He'], ['say,', 'mountains?\"They', \"don't\"])\n",
      "generation:  Should I pack for the ocean, or should I pack for the mountains?\"He says, \"I care.\n",
      "\n",
      "annotation:  Should I pack for the ocean, or should I pack for the mountains?\"They say, \"I don't care.\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for idx in sentence_indices:\n",
    "    print(model_changes['target_changes'][idx])\n",
    "    print('generation: ', generation[idx])\n",
    "    print('annotation: ', target[idx])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
